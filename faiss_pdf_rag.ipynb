{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb65aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "  Installing langchain-community... ‚úÖ\n",
      "  Installing faiss-cpu... ‚úÖ\n",
      "  Installing sentence-transformers... ‚úÖ\n",
      "  Installing python-dotenv... ‚úÖ\n",
      "  Installing tiktoken... ‚úÖ\n",
      "  Installing langchain-groq... ‚úÖ\n",
      "  Installing groq... ‚úÖ\n",
      "  Installing pypdf... ‚úÖ\n",
      "  Installing langchain... ‚úÖ\n",
      "  Installing langchain-text-splitters... ‚úÖ\n",
      "  Installing langchain-core... ‚úÖ\n",
      "  Installing cryptography>=3.1... ‚úÖ\n",
      "  Installing pikepdf... ‚úÖ\n",
      "  Installing unstructured[pdf]... ‚úÖ\n",
      "  Installing pdf2image... ‚úÖ\n",
      "  Installing pdfplumber... ‚úÖ\n",
      "  Installing pillow... ‚úÖ\n",
      "  Installing pytesseract... ‚úÖ\n",
      "\n",
      "‚úÖ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install ALL required libraries (including cryptography)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain-community\",\n",
    "    \"faiss-cpu\",\n",
    "    \"sentence-transformers\",\n",
    "    \"python-dotenv\",\n",
    "    \"tiktoken\",\n",
    "    \"langchain-groq\",\n",
    "    \"groq\",\n",
    "    \"pypdf\",\n",
    "    \"langchain\",\n",
    "    \"langchain-text-splitters\",\n",
    "    \"langchain-core\",\n",
    "    \"cryptography>=3.1\",\n",
    "    \"pikepdf\",\n",
    "    \"unstructured[pdf]\",\n",
    "    \"pdf2image\",\n",
    "    \"pdfplumber\",\n",
    "    \"pillow\",  # Required for image processing\n",
    "    \"pytesseract\",  # For OCR support\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"  Installing {package}...\", end=\" \")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(\"‚úÖ\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è (will continue)\")\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dcfe1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying package installations...\n",
      "  ‚úÖ langchain-groq\n",
      "  ‚úÖ faiss-cpu\n",
      "  ‚úÖ sentence-transformers\n",
      "  ‚úÖ python-dotenv\n",
      "  ‚úÖ tiktoken\n",
      "  ‚úÖ groq\n",
      "  ‚úÖ pypdf\n",
      "  ‚úÖ langchain\n",
      "  ‚úÖ langchain-text-splitters\n",
      "  ‚úÖ langchain-core\n",
      "  ‚úÖ cryptography\n",
      "  ‚úÖ pikepdf\n",
      "  ‚úÖ unstructured\n",
      "  ‚úÖ pdf2image\n",
      "  ‚úÖ pdfplumber\n",
      "  ‚úÖ pillow\n",
      "\n",
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "print(\"Verifying package installations...\")\n",
    "import sys\n",
    "\n",
    "packages_to_check = {\n",
    "    \"langchain_groq\": \"langchain-groq\",\n",
    "    \"faiss\": \"faiss-cpu\",\n",
    "    \"sentence_transformers\": \"sentence-transformers\",\n",
    "    \"dotenv\": \"python-dotenv\",\n",
    "    \"tiktoken\": \"tiktoken\",\n",
    "    \"groq\": \"groq\",\n",
    "    \"pypdf\": \"pypdf\",\n",
    "    \"langchain\": \"langchain\",\n",
    "    \"langchain_text_splitters\": \"langchain-text-splitters\",\n",
    "    \"langchain_core\": \"langchain-core\",\n",
    "    \"cryptography\": \"cryptography\",\n",
    "    \"pikepdf\": \"pikepdf\",\n",
    "    \"unstructured\": \"unstructured\",\n",
    "    \"pdf2image\": \"pdf2image\",\n",
    "    \"pdfplumber\": \"pdfplumber\",\n",
    "    \"PIL\": \"pillow\",\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "for module_name, package_name in packages_to_check.items():\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        print(f\"  ‚úÖ {package_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ùå {package_name} - MISSING\")\n",
    "        missing_packages.append(package_name)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"Installing missing packages...\")\n",
    "    for package in missing_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "            print(f\"  ‚úÖ Installed {package}\")\n",
    "        except:\n",
    "            print(f\"  ‚ùå Failed to install {package}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "270cb2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported available modules\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import available modules\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Import available modules\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"‚úÖ Imported available modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d7fff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating custom implementations for missing modules...\n",
      "‚úÖ Created custom implementations:\n",
      "   - ConversationBufferMemory\n",
      "   - ConversationalRetrievalChain\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create custom implementations for missing modules\n",
    "print(\"Creating custom implementations for missing modules...\")\n",
    "\n",
    "# Custom ConversationBufferMemory\n",
    "class ConversationBufferMemory:\n",
    "    def __init__(self, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\"):\n",
    "        self.memory_key = memory_key\n",
    "        self.return_messages = return_messages\n",
    "        self.output_key = output_key\n",
    "        self.chat_memory = type('obj', (object,), {'messages': []})()\n",
    "        self.buffer = []\n",
    "    \n",
    "    def save_context(self, inputs, outputs):\n",
    "        if \"question\" in inputs and \"answer\" in outputs:\n",
    "            self.buffer.append({\n",
    "                \"question\": inputs[\"question\"],\n",
    "                \"answer\": outputs[\"answer\"]\n",
    "            })\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "        self.chat_memory.messages = []\n",
    "    \n",
    "    def load_memory_variables(self, inputs):\n",
    "        return {self.memory_key: self.buffer}\n",
    "\n",
    "# Custom ConversationalRetrievalChain\n",
    "class ConversationalRetrievalChain:\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm, retriever, memory, verbose=False, return_source_documents=True, get_chat_history=None):\n",
    "        instance = cls()\n",
    "        instance.llm = llm\n",
    "        instance.retriever = retriever\n",
    "        instance.memory = memory\n",
    "        instance.verbose = verbose\n",
    "        instance.return_source_documents = return_source_documents\n",
    "        instance.get_chat_history = get_chat_history if get_chat_history else (lambda h: h)\n",
    "        return instance\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        \n",
    "        # Get chat history from memory\n",
    "        memory_vars = self.memory.load_memory_variables({})\n",
    "        chat_history = memory_vars.get(self.memory.memory_key, [])\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        docs = self.retriever.get_relevant_documents(question)\n",
    "        \n",
    "        # Format chat history\n",
    "        history_text = \"\"\n",
    "        if chat_history:\n",
    "            for item in chat_history[-3:]:  # Last 3 exchanges\n",
    "                if isinstance(item, dict):\n",
    "                    history_text += f\"User: {item.get('question', '')}\\n\"\n",
    "                    history_text += f\"Assistant: {item.get('answer', '')}\\n\\n\"\n",
    "        \n",
    "        # Format context from documents\n",
    "        context = \"\"\n",
    "        for i, doc in enumerate(docs[:3]):  # Use top 3 docs\n",
    "            context += f\"[Document {i+1}]\\n{doc.page_content[:500]}\\n\\n\"\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Based on the following documents and conversation history, answer the question.\n",
    "\n",
    "Previous conversation:\n",
    "{history_text}\n",
    "\n",
    "Relevant documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, concise answer based only on the documents above:\"\"\"\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = self.llm.invoke(prompt)\n",
    "        answer = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        # Save to memory\n",
    "        self.memory.save_context(\n",
    "            {\"question\": question},\n",
    "            {\"answer\": answer}\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"source_documents\": docs if self.return_source_documents else []\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n[DEBUG] Question: {question}\")\n",
    "            print(f\"[DEBUG] Retrieved {len(docs)} documents\")\n",
    "            print(f\"[DEBUG] Answer: {answer[:100]}...\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ Created custom implementations:\")\n",
    "print(\"   - ConversationBufferMemory\")\n",
    "print(\"   - ConversationalRetrievalChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c17f2b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading PDFs from: ./pdfs/\n",
      "Found 4 PDF file(s)\n",
      "Files:\n",
      "  ‚Ä¢ Complete_Guide_YOLO_on_Luckfox_Core1106.pdf\n",
      "  ‚Ä¢ IR Camera.pdf\n",
      "  ‚Ä¢ Rockchip RV1106 Datasheet V1.7-20231218.pdf\n",
      "  ‚Ä¢ YOLOv5_RKNN_Luckfox_Core1106_Guide.pdf\n",
      "\n",
      "üîç Checking for encrypted PDFs...\n",
      "\n",
      "Processing: Complete_Guide_YOLO_on_Luckfox_Core1106.pdf\n",
      "  ‚úÖ Successfully loaded (18 pages)\n",
      "\n",
      "Processing: IR Camera.pdf\n",
      "  ‚úÖ Successfully loaded (9 pages)\n",
      "\n",
      "Processing: Rockchip RV1106 Datasheet V1.7-20231218.pdf\n",
      "  ‚ö†Ô∏è PyPDFLoader failed: cryptography>=3.1 is required for AES algorithm...\n",
      "  üîÑ Trying alternative PDF reader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='./pdfs/Rockchip RV1106 Datasheet V1.7-20231218.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "  ‚úÖ Successfully loaded with UnstructuredPDFLoader (1471 elements)\n",
      "\n",
      "Processing: YOLOv5_RKNN_Luckfox_Core1106_Guide.pdf\n",
      "  ‚úÖ Successfully loaded (14 pages)\n",
      "\n",
      "============================================================\n",
      "üìä Loading Summary:\n",
      "   Total PDFs attempted: 4\n",
      "   Successfully loaded: 4\n",
      "   Failed to load: 0\n",
      "   Total pages loaded: 1512\n",
      "\n",
      "‚úÖ Successfully loaded 1512 document pages\n",
      "\n",
      "üìÑ Sample document:\n",
      "   Source: Complete_Guide_YOLO_on_Luckfox_Core1106.pdf\n",
      "   Page: 1\n",
      "\n",
      "üìù Sample content (first 200 characters):\n",
      "--------------------------------------------------\n",
      "Complete Guide\n",
      "Running YOLO Object Detection\n",
      "on Luckfox Core1106\n",
      "From Windows 11 Setup to Real-Time Inference\n",
      "Date: January 15, 2026\n",
      "Version: 1.0\n",
      "Platform: Luckfox Core1106 (RV1106 SoC)\n",
      "Model: YOLOv5n...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load PDF documents with encryption handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_directory = \"./pdfs/\"  # Using your pdfs folder\n",
    "\n",
    "print(f\"üìÇ Loading PDFs from: {pdf_directory}\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(pdf_directory):\n",
    "    print(f\"‚ùå Directory '{pdf_directory}' not found.\")\n",
    "    print(f\"Creating directory...\")\n",
    "    os.makedirs(pdf_directory, exist_ok=True)\n",
    "    print(f\"‚úÖ Created '{pdf_directory}'\")\n",
    "    print(\"Please add your PDF files to this directory and run this cell again.\")\n",
    "    documents = []\n",
    "else:\n",
    "    # Count PDF files\n",
    "    pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]\n",
    "    print(f\"Found {len(pdf_files)} PDF file(s)\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        print(\"Files:\")\n",
    "        for pdf in pdf_files:\n",
    "            print(f\"  ‚Ä¢ {pdf}\")\n",
    "        \n",
    "        print(\"\\nüîç Checking for encrypted PDFs...\")\n",
    "        \n",
    "        # Try loading each PDF individually to identify problematic ones\n",
    "        successful_docs = []\n",
    "        failed_files = []\n",
    "        \n",
    "        for pdf_file in pdf_files:\n",
    "            pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "            print(f\"\\nProcessing: {pdf_file}\")\n",
    "            \n",
    "            try:\n",
    "                # Try with PyPDFLoader first\n",
    "                from langchain_community.document_loaders import PyPDFLoader\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                file_docs = loader.load()\n",
    "                successful_docs.extend(file_docs)\n",
    "                print(f\"  ‚úÖ Successfully loaded ({len(file_docs)} pages)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è PyPDFLoader failed: {str(e)[:100]}...\")\n",
    "                \n",
    "                # Try with alternative PDF reader\n",
    "                try:\n",
    "                    print(f\"  üîÑ Trying alternative PDF reader...\")\n",
    "                    from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "                    loader = UnstructuredPDFLoader(pdf_path, mode=\"elements\", strategy=\"fast\")\n",
    "                    file_docs = loader.load()\n",
    "                    successful_docs.extend(file_docs)\n",
    "                    print(f\"  ‚úÖ Successfully loaded with UnstructuredPDFLoader ({len(file_docs)} elements)\")\n",
    "                    \n",
    "                except Exception as e2:\n",
    "                    print(f\"  ‚ùå Alternative loader also failed: {str(e2)[:100]}...\")\n",
    "                    \n",
    "                    # Try with pikepdf for encrypted PDFs\n",
    "                    try:\n",
    "                        print(f\"  üîÑ Trying pikepdf...\")\n",
    "                        import pikepdf\n",
    "                        \n",
    "                        # Try to open and decrypt (if possible)\n",
    "                        with pikepdf.open(pdf_path) as pdf:\n",
    "                            print(f\"  ‚úÖ PDF opened with pikepdf ({len(pdf.pages)} pages)\")\n",
    "                            \n",
    "                            # Extract text from each page\n",
    "                            from langchain.schema import Document\n",
    "                            for page_num, page in enumerate(pdf.pages):\n",
    "                                try:\n",
    "                                    text = page.extract_text()\n",
    "                                    if text:\n",
    "                                        doc = Document(\n",
    "                                            page_content=text,\n",
    "                                            metadata={\n",
    "                                                \"source\": pdf_path,\n",
    "                                                \"page\": page_num\n",
    "                                            }\n",
    "                                        )\n",
    "                                        successful_docs.append(doc)\n",
    "                                except:\n",
    "                                    pass\n",
    "                            \n",
    "                            print(f\"  ‚úÖ Extracted text from {len(pdf.pages)} pages\")\n",
    "                            \n",
    "                    except Exception as e3:\n",
    "                        print(f\"  ‚ùå All methods failed for {pdf_file}\")\n",
    "                        print(f\"  Error details: {str(e3)[:150]}\")\n",
    "                        failed_files.append(pdf_file)\n",
    "        \n",
    "        documents = successful_docs\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"üìä Loading Summary:\")\n",
    "        print(f\"   Total PDFs attempted: {len(pdf_files)}\")\n",
    "        print(f\"   Successfully loaded: {len(pdf_files) - len(failed_files)}\")\n",
    "        print(f\"   Failed to load: {len(failed_files)}\")\n",
    "        print(f\"   Total pages loaded: {len(documents)}\")\n",
    "        \n",
    "        if failed_files:\n",
    "            print(f\"\\n‚ùå Failed to load these files:\")\n",
    "            for f in failed_files:\n",
    "                print(f\"   ‚Ä¢ {f}\")\n",
    "            print(\"\\nüí° Solutions:\")\n",
    "            print(\"   1. Try decrypting the PDF with the password\")\n",
    "            print(\"   2. Convert PDF to text format\")\n",
    "            print(\"   3. Use OCR if it's a scanned PDF\")\n",
    "        \n",
    "        if documents:\n",
    "            print(f\"\\n‚úÖ Successfully loaded {len(documents)} document pages\")\n",
    "            \n",
    "            # Show sample\n",
    "            print(f\"\\nüìÑ Sample document:\")\n",
    "            print(f\"   Source: {os.path.basename(documents[0].metadata.get('source', 'Unknown'))}\")\n",
    "            print(f\"   Page: {documents[0].metadata.get('page', 0) + 1}\")\n",
    "            print(f\"\\nüìù Sample content (first 200 characters):\")\n",
    "            print(\"-\" * 50)\n",
    "            sample_text = documents[0].page_content[:200]\n",
    "            if sample_text.strip():\n",
    "                print(sample_text + \"...\")\n",
    "            else:\n",
    "                print(\"[Empty or image-based content - may need OCR]\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"‚ùå No PDF files found in the directory.\")\n",
    "        print(f\"Please add PDF files to '{pdf_directory}' and run this cell again.\")\n",
    "        documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cafdc91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split into 1542 chunks\n",
      "üìä First chunk size: 214 characters\n",
      "üìã First chunk preview:\n",
      "----------------------------------------\n",
      "Complete Guide\n",
      "Running YOLO Object Detection\n",
      "on Luckfox Core1106\n",
      "From Windows 11 Setup to Real-Time Inference\n",
      "Date: January 15, 2026\n",
      "Version: 1.0\n",
      "Plat...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Split documents into chunks\n",
    "if documents:\n",
    "    # Create text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split documents\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    print(f\"‚úÖ Split into {len(docs)} chunks\")\n",
    "    \n",
    "    if docs:\n",
    "        print(f\"üìä First chunk size: {len(docs[0].page_content)} characters\")\n",
    "        print(f\"üìã First chunk preview:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(docs[0].page_content[:150] + \"...\")\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents to split. Please load PDFs in Cell 4 first.\")\n",
    "    docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9780f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "‚úÖ Vector store created successfully!\n",
      "üíæ Vector store saved to 'faiss_index_pdfs'\n",
      "üìä Total vectors in index: 1542\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create embeddings and FAISS vector store\n",
    "if docs:\n",
    "    print(\"Creating embeddings...\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    print(\"Creating vector store...\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    print(\"‚úÖ Vector store created successfully!\")\n",
    "    \n",
    "    # Save the vector store locally\n",
    "    vectorstore.save_local(\"faiss_index_pdfs\")\n",
    "    print(\"üíæ Vector store saved to 'faiss_index_pdfs'\")\n",
    "    \n",
    "    print(f\"üìä Total vectors in index: {vectorstore.index.ntotal}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents to create embeddings. Please run Cells 4-5 first.\")\n",
    "    embeddings = None\n",
    "    vectorstore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd54c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error initializing Groq model: Error code: 400 - {'error': {'message': 'The model `llama3-70b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "Creating a mock LLM for demonstration...\n",
      "‚ö†Ô∏è Using mock LLM. For full functionality, add a valid GROQ API key.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Initialize Groq Llama-3 model\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not groq_api_key:\n",
    "    print(\"‚ö†Ô∏è GROQ_API_KEY not found in .env file\")\n",
    "    print(\"You can:\")\n",
    "    print(\"1. Create a .env file with GROQ_API_KEY=your_key\")\n",
    "    print(\"2. Enter your key below\")\n",
    "    print(\"3. Visit: https://console.groq.com/keys to get an API key\")\n",
    "    \n",
    "    groq_api_key = input(\"Enter your GROQ API key (or press Enter to skip): \")\n",
    "    \n",
    "    if not groq_api_key:\n",
    "        print(\"‚ö†Ô∏è Continuing without API key. Some features will be limited.\")\n",
    "        print(\"You can set the key later with: os.environ['GROQ_API_KEY'] = 'your_key'\")\n",
    "\n",
    "if groq_api_key:\n",
    "    try:\n",
    "        llm = ChatGroq(\n",
    "            groq_api_key=groq_api_key,\n",
    "            model_name=\"llama-3.1-8b-instant\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        # Test the connection with a simple prompt\n",
    "        test_response = llm.invoke(\"Say 'Hello' in one word.\")\n",
    "        print(f\"‚úÖ Groq Llama-3 model initialized successfully!\")\n",
    "        print(f\"ü§ñ Test response: {test_response.content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Groq model: {e}\")\n",
    "        print(\"Creating a mock LLM for demonstration...\")\n",
    "        \n",
    "        class MockLLM:\n",
    "            def invoke(self, prompt):\n",
    "                return type('obj', (object,), {\n",
    "                    'content': f\"[Mock Response] Based on the documents: {prompt[:50]}...\"\n",
    "                })()\n",
    "        \n",
    "        llm = MockLLM()\n",
    "        print(\"‚ö†Ô∏è Using mock LLM. For full functionality, add a valid GROQ API key.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No API key provided. Using mock LLM.\")\n",
    "    \n",
    "    class MockLLM:\n",
    "        def invoke(self, prompt):\n",
    "            return type('obj', (object,), {\n",
    "                'content': f\"[Mock Response - Add GROQ API Key] {prompt[:50]}...\"\n",
    "            })()\n",
    "    \n",
    "    llm = MockLLM()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
